- GPT and most other LLMs that came after placed the normalization layers *before* the attention and FeedForward modules (known as Pre-LN or Pre-Norm)
    - [2020, Xiong et al.](https://arxiv.org/abs/2002.04745) showed that Pre-LN results in more well-behaved gradients at initialization. Furthermore, the researchers mentioned that Pre-LN even works well without careful learning rate warm-up, which is otherwise a crucial tool for Post-LN.
    - OLMO2 normalization layers are still inside the residual layers (skip connections) (even though they are after attention and FF layers, as in OG transformers.)
    - noteworthy OLMo 2 architecture (MHA not GQA) design decisions are primarily the RMSNorm placements
        - 32B OLMO2 variant uses GQA
    - Llama 4
    - Qwen3 ->Qwen 3 is a foundation model but not considered frontier
        - doesn’t push latest boundaries
        - why did they move away from shared expert
        - also only major model not using NoPE or hybrid
    - NoPE → most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, [are not well suited](https://arxiv.org/abs/2305.19466) for length generalization in downstream tasks.
        - attention weights are adjusted to remove dependence on explicit encodings, often by normalizing or constraining QK interactions.
    - self-attention treats tokens independently of order !!!
    - Kimi K2
        - uses [Muon](https://github.com/KellerJordan/Muon) optimizer over AdamW
