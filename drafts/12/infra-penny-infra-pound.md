# (How) Trading Resilience for Convenience Created the Autonomous Architecture Mandate

On November 18, 20% of the global web went dark for three hours. Not from a cyberattack. Not from physical infrastructure failure. A single .unwrap() call in Cloudflare’s Rust codebase brought down X, OpenAI, Discord, and critical infrastructure across five continents. The trigger was a database permission change. The catalyst was a SQL query missing a WHERE clause, returning double the expected data. The crash was a hard-coded limit in a panicky edge worker that didn’t  degrade gracefully.

The day before, Microsoft Azure’s Front Door collapsed after a small change to its load balancer. Services went dark globally for two hours. The week prior, AWS suffered a control-plane outage that took down over 3,500 companies when a race condition in an internal orchestration system deadlocked during routine deployment. Three weeks, three Tier 1 infrastructure providers, three small changes that cascaded into existential failures for companies that thought they’d architected for resilience. 

Digital Ocean users discovered upstream Cloudflare exposure when their services (Spaces CDN, App Platform, and global load balancers) went offline simultaneously, a dependency quietly mentioned in their subprocessors page. GitHub Ops went down hours after Cloudflare recovered, tracing back to a shared dependency on Cloudflare’s edge network for DDoS mitigation. Hidden upstream and transitive dependencies are everywhere. Railway.com launched an object storage service that’s simply a wrapper for Wasabi buckets, a fact not mentioned anywhere, not even on their subprocessors page. Hidden upstream dependencies are everywhere in modern infrastructure, and most CTOs don’t discover them until the blast radius reaches their dashboards.

This quarter’s cluster of outages across AWS, Azure, Cloudflare, and GitHub didn’t teach us anything new but reminded us of something we’ve spent a decade pretending wasn’t true. We’ve built a world where the smallest operational decision can have the biggest operational blast radius. Not because the failures were exotic. Not because a nation-state attacker slipped through a zero-day. But because of something much harder to defend against: routine changes traveling through systems that are no longer routine.

This is the new failure mode: not direct dependencies but transitive ones, where your infrastructure’s infrastructure fails and only you find out why by reading incident reports on HN. You saved pennies by outsourcing infrastructure to hyperscale providers. Now you’re in for a pound when their cascading failures take down your entire operation. What’s changed is the scale at which “normal engineering” now operates. Choosing cloud infrastructure means your survival depends on it. The cost savings from abstraction get repriced 10x or 100x when outages compound across your stack. CTOs need to commit to architectural patterns that follow through when dependencies fail, which means building for resilience isn’t optional anymore. 

As if it ever was. 

❝ The Internet was designed to survive a nuclear fallout. On November 18, 2025, it couldn’t survive a missing WHERE clause. ❞

Context and Background

Except the Internet wasn’t designed to survive a nuclear fallout. Charles Herzfeld, ARPA director during ARPANET’s inception, put it bluntly:  ”The goal of ARPANET was to facilitate communication and resource sharing between researchers and institutions. While there has been speculation ARPANET was prompted by the need for a reliable and decentralized system that could withstand nuclear attacks, that is false.” Akshully, the true goal was addressing a practical reality: expensive, scarce computing resources needed to be shared among geographically separate research institutions. This goal is often confused with conceptual work on resilient communication by Paul Baran at RAND in the 1960s, which study predates and was entirely separate from ARPANET’s development, and provided the genesis for an enduring, conspiratorial urban legend.

Aside from Baran’s high-level work at RAND, none of the packet switching pioneers were responding to military survivability requirements. Donald Davies at NPL in the former UK, Leonard Kleinrock MIT developing the mathematical theory for his 1962 MIT dissertation, the ARPANET teams, (also the CYCLADES group in France) all focused on resource sharing and research networking. The US military developed its own techniques for redundancy (SAGE network) but we’d have to wait until 1983 for a genuinely military packet-switched network: the Defense Data Network / MILNET.

Packet switching doesn’t magically remove central infrastructure. It only removes the requirement for a single, unique central switch. In practice you still have the  specialized intermediate nodes familiar to network engineers (routers, gateways, IXPs, core routers), hierarchical routing, and centralized services (DNS hierarchy, BGP route reflectors, large backbones, CDNs, cloud providers.) Outages at Cloudflare, Azure, and AWS are not flaws in Internet protocols themselves but in particular service architectures deployed on top of them.

We didn’t lose the distributed Internet in November. We just stopped using it.

Industry Impact

Inverting the Design Goal

We traded resilience for convenience, concentrating 50% of the web’s traffic into three vendors. Cloudflare handles roughly a fifth of global web traffic. When their proxy code calls .unwrap() on a Result for an operation expected to never fail (ha) and that expectation proves wrong, that’s a fifth of the Internet potentially going dark. Azure’s Front Door routes traffic for millions of enterprise applications. When a configuration change to its load balancer triggers a cascading failure, those applications don’t fail gracefully, they disappear. AWS’s control plane orchestrates infrastructure for, um, a significant fraction, of the modern web. When a race condition deadlocks during routine deployment, over 3,500 companies simultaneously lose access to their own infrastructure. We optimized for cost, and quietly rebuilt the Internet to behave like a single, giant, fragile mainframe whose health depends on whether one engineer, somewhere, gets one line of config right.

The year-end promotion cycle amplifies this risk. Changes get pushed with less diligence, reviewed with less scrutiny, and deployed with less caution bc everyone’s focused on closing out their performance metrics before the ball drops.  The cultural DNA here misaligns organizational incentives with operational stability right when traffic peaks for holiday shopping, year-end closes, and Q4 revenue recognition. It’s everything at once, like the web’s own epagomenal days, a period outside ordinary time when the margin for error vanishes and every latent flaw get its reckoning. (This also explains why the holidays overlap more and more, but don’t get me started.)

Such speed is a double-edged sword. It drives world-class agility: we see organizations like Cloudflare where co-founders are reviewing customer-reported product defects within ten minutes and shipping fixes the next day, an unheard-of velocity for a company of that magnitude. Yet, this same institutional velocity allows the catastrophic deployment of a change containing an uncaught exception (that unwrap() panic) that takes down a fifth of the global internet. Happy holidays. 

This isn’t “move fast and break things” as an ethos, but a culture where moving fast means things will occasionally break catastrophically. (Don’t get me started on dromology.) The critical question for CTOs isn’t whether to optimize for velocity or stability, but how to ensure your organization’s impressive speed in shipping features and patching defects doesn’t also manifest in an existential operational lapse. The infrastructure providers you depend on are making the same tradeoff you are, highlighting a fundamental fragility in the hyperscale model, except their failures cascade across thousands of customers. Tier 3 Cloud providers capture margin by abstracting complexity, but that abstraction often creates centralized control planes that can fail catastrophically. When AWS’s control plane goes down, you lose infra access even though the underlying compute is fine. When Cloudflare’s edge worker panics, it’s not just their problem but yours, your customers’ and everyone downstream who didn’t read the subprocessors page. 

 The infrastructure substrate needs to support autonomous operations because the centralized control planes we’ve been depending on can’t absorb the operational complexity

KubeCon 2025 crystallized what the outages made implicit: K8s adoption is accelerating precisely because it’s the hedge against Tier 3 provider failures. The infrastructure substrate needs to support autonomous operations because centralized control planes cannot absorb the operational complexity of modern workloads not to even mention the impending but already ongoing hyperscale AI workloads create. Kubernetes isn’t winning because it’s technically superior. It’s winning because it’s the only widely-adopted substrate that provides the operational primitives both resilient architecture and agentic AI and require: dynamic resource allocation, declarative configuration that survives control-plane failures, and RBAC policies that can constrain autonomous actors.

This connects directly to the Tier 3 economics described above. Cloud providers (Tier 3) are capturing margin by abstracting infrastructure complexity, but that abstraction creates single points of failure. When AWS’s control plane goes down, you lose access to your infrastructure even though the underlying compute is fine. When Azure’s Front Door misconfigures, your applications become unreachable even though your services are running. The Tier 3 business model is predicated on centralized control planes that can fail catastrophically, which explains Kubernetes adoption is accelerating: it’s the hedge against Tier 3 provider failures because the control plane runs in your infrastructure, not theirs.

Three trends converged at KubeCon 2025 that highlight this transformation.

AI Workflows Go Operational: Agentic workflows moved from experimental to operational, which means they now schedule their own compute and provision their own resources. This requires an infrastructure that can safely delegate operational control to non-human actors, making Kubernetes’ declarative model and RBAC primitives safety-critical not just organizational convenience.

Platform Engineering Graduates: Platform engineering has graduated from buzzword to discipline; its no longer about DX, but building robust guardrails that prevent your engineers, or your agents, from committing the equivalent of Cloudflare’s .unwrap() incident. Deprecations of ingress-nginx forces devops teams to confront whether they understood their actual ingress requirements or just copy-pasted manifests from SO. Helm 4.0.0 shipped with breaking changes that exposed which teams treated it as a package manager (correct) versus config management (wrong). Teams that understood the distinction migrated smoothly; teams that didn’t spent November firefighting. 

Observability Shifts to Insight: Traditional observability (logs, dashboards, monitoring) was built for predictable systems operated by humans. Agentic systems are neither. When an agent scales your cluster, you need to know why it decided chosen value was correct, whether that decision aligned with your cost constraints, and what would have happened if that target was set higher or lower. Tooling is evolving from “what happened” to why did the system decide this was the right action, which is the only way to trust autonomous operations. During the November outages, cloud providers were briefly flying blind because their observability depended on the same planes that were failing. Medusa Cascade. The next generation of ops requires observability that remains visible even when the system’s own sensors go dark. Let’s hope it’s not the naked leading the blind. 

Pulumi versus OpenTofu highlights the infrastructure-as-code implications. OpenTofu is declarative, generally human-authored, IAC. Pulumi is imperative, programmatic, and increasingly optimised for codegen. For human operators, OpenTofu’s declarative model is easier to reason about. For AI agents, Pulumi’s programmatic interface is the only model that works because agents can’t meaningfully generate declarative HCL, but they can generate TypeScript or Python that calls Pulumi’s SDK. Companies betting on AI-native SDLC are standardizing on Pulumi because it’s the only infrastructure-as-code paradigm that survives contact with autonomous agents generating infrastructure definitions. (But frankly, things could have turned out much differently if the big BLC brouhaha had not happened.) 

What This Means For CTOs

Architecting for Dependency Resilience

The consolidation around three hyperscale infrastructure providers creates systemic risk that most CTOs haven’t priced correctly. When a fifth of the global web depends on Cloudflare’s edge network, Cloudflare’s outages become everyone’s outages. When Azure Front Door routes traffic for millions of enterprise apps, Azure’s configuration mistakes become your revenue loss. The distributed Internet still exists, but we stopped using it because abstraction layers made centralization cheaper and faster than running your own infrastructure (allowing companies to focus more resources on their respective business domains, changing the face of software consultancy.)

This isn’t a call to repatriate to on-prem datacenters. It’s recognition that the infrastructure layer’s increasing fragility changes the ROI calculation for redundancy. Multi-cloud used to be expensive insurance that most companies couldn’t justify. After three Tier 1 outages in three weeks, it’s starting to look like operational necessity. (Assuming you have your control plane issues sorted.) The companies that architected for single-provider dependence are repricing that decision now that the blast radius is measured in hours of lost revenue, not hypothetical risk scenarios.

The agentic infrastructure shift amplifies this pressure. When AI agents are provisioning resources, scaling workloads, and making deployment decisions, the failure modes multiply. A human operator making a mistake triggers one incident. An agent making a mistake triggers hundreds of incidents simultaneously because it’s operating at machine speed across your entire fleet. The infrastructure patterns that worked for human-operated systems (manual approval gates, change management processes, slow rollout schedules) don’t translate to agentic operations. You either build automated guardrails that can constrain agent behavior without blocking legitimate actions, or you accept that agent-authored infrastructure changes will periodically take down production in novel and spectacular ways.

Platform engineering’s maturation is the industry’s response to this complexity. Teams that treated platform engineering as “DevOps with a new name” are struggling. The teams that built actual platforms (internal abstractions that hide infrastructure complexity while maintaining operational control) are shipping faster with fewer outages. Kubernetes, Terraform and Helm (which just celebrated its 4.0 release) are primitives, not platforms. The platform is what you build on top of them that lets your engineers and your agents deploy safely without needing to understand the underlying infrastructure.

You’re not architecting for resilience against infrastructure failure anymore. You’re architecting for resilience against infrastructure dependency. The outages at Cloudflare, Azure, and AWS weren’t infrastructure failures but dependency failures, where small changes cascaded through systems that had become too centralized to fail gracefully.

CTO Playbook: 30/60/90

Pre-generate Cloudflare API tokens (CRITICAL: Immediate) If you use Cloudflare, generate backup API tokens now and store them in a separate secret management system. When Cloudflare’s control plane goes down, you can’t generate new tokens, which means you can’t route around the failure. Teams without pre-generated tokens were locked out for three hours in November.

Map transitive dependencies (HIGH PRIORITY: EOY) Audit your infrastructure for hidden upstream dependencies. Check your CDN provider’s subprocessors page. Verify your managed database doesn’t route through a shared dependency. Assume every managed service has upstream dependencies the vendor didn’t disclose.

Test failover paths (MEDIUM-HIGH PRIORITY: 45 days ) Run tabletop exercises where your primary cloud provider is unavailable for four hours. Can you route traffic to backup regions? Can you fail over databases? Can you deploy without access to your CI/CD system? If the answer is no, you don’t have a failover plan but a hope-based strategy.

Kubernetes migration plan (MEDIUM PRIORITY: 60 days) If you’re not running Kubernetes, create a migration roadmap. If you are running Kubernetes, audit whether your manifests are agent-compatible: declarative, versioned, and safe for programmatic generation.

Observability upgrade (MEDIUM PRIORITY: 60 days) Implement decision tracing for any autonomous systems. Logs and metrics aren’t enough when agents are making operational decisions. You need instrumentation that captures why the agent chose a specific action, not just what action it took.

Infrastructure-as-code audit (MEDIUM PRIORITY: 60 days) If you’re on Terraform/OpenTofu and planning to integrate agents, evaluate Pulumi. The migration cost is real, but it’s lower today than it will be in six months when your entire stack is codified in HCL that agents can’t meaningfully generate.

Multi-cloud failover (LOW PRIORITY: 90- days) Architect critical paths for multi-provider redundancy. This doesn’t mean running everything on AWS and Azure simultaneously. It means ensuring your most critical workloads can fail over to a different provider without manual intervention.

Platform engineering investment (LOW PRIORITY: 90 days) If you don’t have an internal platform team, build one. If you have one, audit whether they’re building abstractions or just managing infrastructure. The goal is enabling engineers and agents to deploy safely without understanding underlying infrastructure complexity.

Vendor concentration risk analysis (LOW PRIORITY: 90 days) Calculate what percentage of your infrastructure depends on a single provider. If it’s over 70%, you have existential concentration risk. Document alternatives, migration costs, and failover timelines. Present to your board before they ask why a Cloudflare outage cost you hours and hours of revenue.

Bottom Line

Your immediate decision is whether to accept single-provider risk or architect for multi-provider redundancy. Multi-cloud used to be expensive insurance most companies skipped. After November 2025, it’s the only architecture that survives Tier 1 provider outages. This doesn’t mean running identical infrastructure across AWS and Azure. It means architecting workloads so critical paths can route around provider failures without manual intervention. Static sites can fail over to different CDNs. APIs can route to backup regions on different cloud providers. Databases can maintain hot replicas across providers for failover scenarios. The cost is higher. The operational complexity is higher. The alternative is explaining to your board why a WHERE clause at Cloudflare took down your revenue for half a day.

The agentic infrastructure requirements are orthogonal but equally urgent. If you’re not already running Kubernetes, consider a migration plan. Not because Kubernetes is perfect, but because it’s the only substrate with enough adoption, tooling maturity, and operational primitives to safely run agentic workloads at scale. Teams still hand-rolling infrastructure or running on managed platforms will hit scaling limits the moment they try to deploy agents that need dynamic resource allocation across heterogeneous hardware. Which is ofc one of the main points of agents. 

Your observability must evolve from “what happened” to “why did the system decide this.” Traditional monitoring tells you your inference cluster scaled to 100 nodes. Agent-aware observability tells you the agent predicted demand based on historical patterns, chose 100 nodes to maintain latency SLAs, and stayed within budget. Without decision traces, you can’t trust autonomous operations, which means you can’t delegate operational control to agents, which means you’re stuck with human-operated systems that can’t scale at the pace AI workloads require.

Either way, you’re in for a pound. 

To attend CTO Lunches, please register at ctolunches.com and choose NYC as your city.

Subscribe for free to receive monthly updates from CTO Lunch NYC
